{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from random import *\n",
    "\n",
    "# SK general \n",
    "from sklearn import model_selection\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# AutoML Libraries tpot and auto-sklearn\n",
    "from tpot import TPOTClassifier\n",
    "# from autosklearn.classification import AutoSklearnClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Annealing function steps\n",
    "# @parameters \n",
    "# sol - random solution (ML model)\n",
    "# X_train, y_train - training data\n",
    "# X_test, y_test - testing data\n",
    "# @return solution and cost\n",
    "\n",
    "# 1. Generate a random solution\n",
    "# 2. Calculate its cost using a cost function (accuracy of the ML Model)\n",
    "# 3. Generate a random neigboring solution\n",
    "# 4. Calculate new solutions cost (accuracy of the ML model)\n",
    "# 5. Compare solutions\n",
    "#     - If c_new > c_old move to the new solution\n",
    "#     - If c_new < c_old maybe move to the new solution\n",
    "# 6. Repeat steps until an acceptable solution is found or max number of iterations is reached\n",
    "def sa(sol, X_train, y_train, X_test, y_test):\n",
    "    t = 1.0\n",
    "    t_min = 0.00001\n",
    "    alpha = 0.9\n",
    "    old_cost = cost(sol, X_train, y_train, X_test, y_test)\n",
    "    while t > t_min:\n",
    "        i = 1\n",
    "        while i <= 100:\n",
    "            new_sol = neighbor(sol)\n",
    "            new_cost = cost(new_sol, X_train, y_train, X_test, y_test)\n",
    "            ap = acceptance_probability(old_cost, new_cost, t)\n",
    "            rnd = random()\n",
    "            # print(ap, rnd)\n",
    "            if ap > rnd:\n",
    "                sol = new_sol\n",
    "                old_cost = new_cost\n",
    "            i += 1\n",
    "        t = t * alpha\n",
    "    return sol, old_cost          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate neighboring solution\n",
    "# Solution is defined as a machine learning model along with a set of parameters\n",
    "# i.e.,\n",
    "# solution = {\n",
    "#     model: 'LinearRegression',\n",
    "#     parameters: {\n",
    "#         fit_intercept: true,\n",
    "#         normalize: true,\n",
    "#         copy_X: false,\n",
    "#         n_jobs: 4,\n",
    "#     }\n",
    "# }\n",
    "def neighbor(sol):\n",
    "    # Use the search_space to find a new neighbor of the current solution and return that\n",
    "    \n",
    "    # Chose a random parameter and modify it \n",
    "    parameter = choice(list(search_space[sol['name']].keys())) # random.choice()\n",
    "    # print('Selected parameter ', parameter)\n",
    "    \n",
    "    parameter_is_tuple = type(search_space[sol['name']][parameter]) is tuple\n",
    "    \n",
    "    if parameter_is_tuple:\n",
    "        parameter_space = search_space[sol['name']][parameter][0]\n",
    "    else:\n",
    "        parameter_space = search_space[sol['name']][parameter]\n",
    "    # print('Parameter space', parameter_space)\n",
    "    \n",
    "    # Grab the random parameter from our current solution and change it\n",
    "    if parameter_is_tuple:\n",
    "        current_parameter_val = sol['parameters'][parameter][0]\n",
    "    else:\n",
    "        current_parameter_val = sol['parameters'][parameter]\n",
    "    # print('Current parameter value', current_parameter_val)\n",
    "    \n",
    "    # Grab the current index of the selected parameter of our model\n",
    "    current_index = parameter_space.index(current_parameter_val)\n",
    "    \n",
    "    # In one step modify the value of the selected parameter\n",
    "    if current_index == 0:\n",
    "        # index = 0 -> index++\n",
    "        if parameter_is_tuple:\n",
    "            modified_parameter = (parameter_space[1],)\n",
    "        else:\n",
    "            modified_parameter = parameter_space[1];\n",
    "    elif current_index == len(parameter_space):\n",
    "        # index = length -> index--\n",
    "        if parameter_is_tuple:\n",
    "            modified_parameter = (parameter_space[current_index - 1],)\n",
    "        else:\n",
    "            modified_parameter = parameter_space[current_index - 1]\n",
    "    else: \n",
    "        # index = index + random(-1,1)\n",
    "        if parameter_is_tuple:\n",
    "             modified_parameter = (parameter_space[(current_index + choice([-1, 1])) % len(parameter_space)],)\n",
    "        else:\n",
    "            modified_parameter = parameter_space[(current_index + choice([-1, 1])) % len(parameter_space)]\n",
    "    \n",
    "    # Create a new solution copy the current one and replace the randomly chosen parameter\n",
    "    new_sol = sol\n",
    "    new_sol['parameters'][parameter] = modified_parameter\n",
    "    \n",
    "    # print('New parameter value', modified_parameter)\n",
    "    \n",
    "    # Return new neighboring solution\n",
    "    return new_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the cost of a current solution\n",
    "# In our case the cost is the accuracy (or we can use other metrics) of the current ML model / parameter configuration\n",
    "# TODO: RMSE or some other metric?\n",
    "def cost(sol, X_train, y_train, X_test, y_test): \n",
    "    # Get model\n",
    "    model = get_model(sol['name'], sol['parameters'])\n",
    "    \n",
    "    # Train model on data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    # predictions = model.predict(X_test)\n",
    "    \n",
    "    # Get accuracy or some other metric and return\n",
    "    score = model.score(X_test, y_test)\n",
    "    # print('Score: ', score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a model with a parameter configuration\n",
    "def get_model(name, parameters):\n",
    "    # LogisticRegression\n",
    "    if name == 'LogisticRegression':\n",
    "        lr = LogisticRegression(**parameters)\n",
    "        return lr\n",
    "    # Model2\n",
    "    elif name == 'MLPClassifier':\n",
    "        abc = MLPClassifier(**parameters)\n",
    "        return abc\n",
    "    # Model3\n",
    "    elif name == 'SGDClassifier':\n",
    "        sgd = SGDClassifier(**parameters)\n",
    "        return sgd\n",
    "    # Model4\n",
    "    elif name == 'SVC':\n",
    "        svc = SVC(**parameters)\n",
    "        return svc\n",
    "    # Model5\n",
    "    elif name == 'RandomForestClassifier':\n",
    "        rfc = RandomForestClassifier(**parameters)\n",
    "        return rfc\n",
    "    # Default\n",
    "    else:\n",
    "        print('No model provided')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which recommends if we should jump to a new solutions or not\n",
    "# 1.0 - definitely switch\n",
    "# 0.0 - definitely stay put\n",
    "# 0.5 - 50/50 odds of switching\n",
    "# Usually calculated by e^((c_new - c_old)/t)\n",
    "\n",
    "def acceptance_probability(old_cost, new_cost, t): \n",
    "    if new_cost > old_cost:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return np.exp((new_cost - old_cost)/t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Space that will be used to define our neighborhood of ML models and hyperparameters\n",
    "# Basically our dictionary defining the model, its most important parameters, and their value ranges\n",
    "search_space = {\n",
    "    'LogisticRegression': {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': np.logspace(-4, 4, 20).tolist(),\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'hidden_layer_sizes': (np.arange(1, 100, 1).tolist(), ),\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'alpha': np.logspace(-4, 1, 10).tolist(),\n",
    "        'learning_rate': ['constant', 'invscaling', 'adaptive'],       \n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'alpha': np.logspace(-4, 1, 10).tolist(),\n",
    "        'eta0': np.logspace(-4, 1, 10).tolist(),\n",
    "        'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': np.logspace(-4, 1, 10).tolist(),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': np.arange(1, 100, 1).tolist(),\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': np.arange(2, 50, 1).tolist(),\n",
    "        'min_samples_split': np.arange(0.1, 1.1, 0.1).tolist()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handwritten Digits\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine Quality Data 3\n",
    "wwine = pd.read_csv(\"./data/winequality-white.csv\", sep=';')\n",
    "labels = [\"poor\", \"average\", \"excellent\"]\n",
    "wwine['class'] = pd.cut(wwine['quality'], bins = 3, labels = labels)\n",
    "wwine['class'] = pd.cut(wwine['quality'], bins = 3, labels = False)\n",
    "\n",
    "X = wwine[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']]\n",
    "y = wwine['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.33,random_state=123)\n",
    "\n",
    "# standadise\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some random solution that we will pass to the sa() to start with\n",
    "models = []\n",
    "\n",
    "# LogisticRegression\n",
    "models.append({\n",
    "    'name': 'LogisticRegression',\n",
    "    'parameters': {\n",
    "        'penalty': 'l2',\n",
    "        'C': 0.23357214690901212,\n",
    "        'solver': 'liblinear',\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "})\n",
    "\n",
    "# MLPClassifier\n",
    "models.append({\n",
    "    'name': 'MLPClassifier',\n",
    "    'parameters': {\n",
    "        'hidden_layer_sizes': (1,),\n",
    "        'activation': 'relu',\n",
    "        'solver': 'adam',\n",
    "        'alpha': 0.0001,\n",
    "        'learning_rate': 'constant',       \n",
    "    }\n",
    "})\n",
    "\n",
    "# # SGDClassifier\n",
    "models.append({\n",
    "    'name': 'SGDClassifier',\n",
    "    'parameters': {\n",
    "        'penalty': 'l1',\n",
    "        'loss': 'hinge',\n",
    "        'alpha': 0.0001,\n",
    "        'eta0': 0.0001,\n",
    "        'learning_rate': 'constant'\n",
    "    }\n",
    "})\n",
    "\n",
    "# # SVC\n",
    "models.append({\n",
    "    'name': 'SVC',\n",
    "    'parameters': {\n",
    "        'C': 0.0001,\n",
    "        'kernel': 'linear',\n",
    "        'gamma': 'scale',\n",
    "    }\n",
    "})\n",
    "\n",
    "# RandomForest\n",
    "models.append({\n",
    "    'name': 'RandomForestClassifier',\n",
    "    'parameters': {\n",
    "        'n_estimators': 1,\n",
    "        'criterion': 'gini',\n",
    "        'max_depth': 2,\n",
    "        'min_samples_split': 0.1,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with  LogisticRegression\n",
      "Done with  MLPClassifier\n",
      "Done with  SGDClassifier\n",
      "Done with  SVC\n",
      "Done with  RandomForestClassifier\n",
      "Best solution:  {'solution': {'name': 'MLPClassifier', 'parameters': {'hidden_layer_sizes': (68,), 'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.016681005372000592, 'learning_rate': 'constant'}}, 'score': 0.7520098948670377}\n",
      "All solutions [{'solution': {'name': 'LogisticRegression', 'parameters': {'penalty': 'l1', 'C': 10000.0, 'solver': 'liblinear', 'n_jobs': -1}}, 'score': 0.7235621521335807}, {'solution': {'name': 'MLPClassifier', 'parameters': {'hidden_layer_sizes': (68,), 'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.016681005372000592, 'learning_rate': 'constant'}}, 'score': 0.7520098948670377}, {'solution': {'name': 'SGDClassifier', 'parameters': {'penalty': 'l1', 'loss': 'squared_loss', 'alpha': 0.004641588833612782, 'eta0': 0.0001, 'learning_rate': 'invscaling'}}, 'score': 0.7309833024118738}, {'solution': {'name': 'SVC', 'parameters': {'C': 0.001291549665014884, 'kernel': 'poly', 'gamma': 'auto'}}, 'score': 0.7421150278293135}, {'solution': {'name': 'RandomForestClassifier', 'parameters': {'n_estimators': 28, 'criterion': 'gini', 'max_depth': 15, 'min_samples_split': 0.1}}, 'score': 0.7396413110698825}]\n"
     ]
    }
   ],
   "source": [
    "# Save all solutions\n",
    "solutions = []\n",
    "\n",
    "# Save best solution\n",
    "best_solution = {\n",
    "    'solution': '',\n",
    "    'score': 0\n",
    "}\n",
    "\n",
    "# Iterate over models and get the best solution / score\n",
    "for i in models:\n",
    "    solution, score = sa(i, X_train, y_train, X_test, y_test)\n",
    "    print('Done with ', solution['name'])\n",
    "    solutions.append({\n",
    "       'solution': solution,\n",
    "       'score': score\n",
    "    })\n",
    "    \n",
    "    if score > best_solution['score']:\n",
    "        best_solution = {\n",
    "            'solution': solution,\n",
    "            'score': score\n",
    "        }\n",
    "    \n",
    "# Print results\n",
    "print('Best solution: ', best_solution)\n",
    "print('All solutions', solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n",
      "Score:  0.769325912183055\n"
     ]
    }
   ],
   "source": [
    "# Get TPOT classification optimizer\n",
    "tpot_automl = TPOTClassifier(generations = 5, population_size = 20, cv = 5, random_state = 42, n_jobs = -1)\n",
    "\n",
    "# Fit on dataset\n",
    "tpot_automl.fit(X_train, y_train)\n",
    "\n",
    "# Get score\n",
    "print('Score: ', tpot_automl.score(X_test, y_test))\n",
    "tpot_automl.export('tpot_whitewine_pipeline.py')\n",
    "# tpot_automl.export('tpot_iris_pipeline.py')\n",
    "# tpot_automl.export('tpot_digits_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2020-01-24 10:36:47,651:EnsembleBuilder(1):ed217d05461c6dff1ebbcd34c23a0766] No models better than random - using Dummy Score!\n",
      "[WARNING] [2020-01-24 10:36:47,671:EnsembleBuilder(1):ed217d05461c6dff1ebbcd34c23a0766] No models better than random - using Dummy Score!\n",
      "Score:  1.0\n",
      "auto-sklearn results:\n",
      "  Dataset name: ed217d05461c6dff1ebbcd34c23a0766\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.971429\n",
      "  Number of target algorithm runs: 3168\n",
      "  Number of successful target algorithm runs: 2928\n",
      "  Number of crashed target algorithm runs: 240\n",
      "  Number of target algorithms that exceeded the time limit: 0\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n",
      "[(1.000000, SimpleClassificationPipeline({'balancing:strategy': 'none', 'categorical_encoding:__choice__': 'no_encoding', 'classifier:__choice__': 'k_nearest_neighbors', 'imputation:strategy': 'mean', 'preprocessor:__choice__': 'liblinear_svc_preprocessor', 'rescaling:__choice__': 'minmax', 'classifier:k_nearest_neighbors:n_neighbors': 2, 'classifier:k_nearest_neighbors:p': 2, 'classifier:k_nearest_neighbors:weights': 'distance', 'preprocessor:liblinear_svc_preprocessor:C': 418.0268755058258, 'preprocessor:liblinear_svc_preprocessor:dual': 'False', 'preprocessor:liblinear_svc_preprocessor:fit_intercept': 'True', 'preprocessor:liblinear_svc_preprocessor:intercept_scaling': 1, 'preprocessor:liblinear_svc_preprocessor:loss': 'squared_hinge', 'preprocessor:liblinear_svc_preprocessor:multi_class': 'ovr', 'preprocessor:liblinear_svc_preprocessor:penalty': 'l1', 'preprocessor:liblinear_svc_preprocessor:tol': 0.015979157427287432},\n",
      "dataset_properties={\n",
      "  'task': 2,\n",
      "  'sparse': False,\n",
      "  'multilabel': False,\n",
      "  'multiclass': True,\n",
      "  'target_type': 'classification',\n",
      "  'signed': False})),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Get auto-sklearn classifier\n",
    "sklearn_automl = AutoSklearnClassifier()\n",
    "\n",
    "# Fit on dataset\n",
    "sklearn_automl.fit(X_train, y_train)\n",
    "\n",
    "# y_hat = sklearn_automl.predict(X_test)\n",
    "\n",
    "# Get score\n",
    "print('Score: ', sklearn_automl.score(X_test, y_test))\n",
    "print(sklearn_automl.sprint_statistics())\n",
    "print(sklearn_automl.show_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris data output\n",
    "# Done with  LogisticRegression\n",
    "# Done with  MLPClassifier\n",
    "# Done with  SGDClassifier\n",
    "# Done with  SVC\n",
    "# Done with  RandomForestClassifier\n",
    "# Best solution\n",
    "iris_best = {\n",
    "    'solution': {\n",
    "        'name': 'LogisticRegression', \n",
    "        'parameters': {\n",
    "            'penalty': 'l2', \n",
    "            'C': 0.0018329807108324356, \n",
    "            'solver': 'saga', \n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    }, \n",
    "    'score': 1.0\n",
    "}\n",
    "#All solutions\n",
    "iris_all = [{\n",
    "    'solution': {\n",
    "        'name': 'LogisticRegression', \n",
    "        'parameters': {\n",
    "            'penalty': 'l2', \n",
    "            'C': 0.0018329807108324356, \n",
    "            'solver': 'saga', \n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    }, \n",
    "    'score': 1.0\n",
    "}, \n",
    "{\n",
    "    'solution': {\n",
    "        'name': 'MLPClassifier', \n",
    "        'parameters': {\n",
    "            'hidden_layer_sizes': (48,), \n",
    "            'activation': 'tanh', \n",
    "            'solver': 'sgd', \n",
    "            'alpha': 0.05994842503189409, \n",
    "            'learning_rate': 'constant'\n",
    "        }\n",
    "    }, \n",
    "    'score': 1.0\n",
    "}, \n",
    "{\n",
    "    'solution': {\n",
    "        'name': 'SGDClassifier', \n",
    "        'parameters': {\n",
    "            'penalty': 'l1', \n",
    "            'loss': 'modified_huber', \n",
    "            'alpha': 0.001291549665014884, \n",
    "            'eta0': 0.0001, \n",
    "            'learning_rate': 'adaptive'\n",
    "        }\n",
    "    }, \n",
    "    'score': 1.0\n",
    "}, \n",
    "{\n",
    "    'solution': {\n",
    "        'name': 'SVC', \n",
    "     'parameters': {\n",
    "         'C': 0.05994842503189409, \n",
    "         'kernel': 'poly', \n",
    "         'gamma': 'scale'\n",
    "     }\n",
    "    }, \n",
    "    'score': 1.0\n",
    "}, \n",
    "{\n",
    "    'solution': {\n",
    "        'name': 'RandomForestClassifier', \n",
    "        'parameters': {\n",
    "             'n_estimators': 82, \n",
    "             'criterion': 'gini', \n",
    "             'max_depth': 9, \n",
    "             'min_samples_split': 0.1\n",
    "        }\n",
    "    }, \n",
    "    'score': 1.0\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digits dataset output\n",
    "digits_best =  {\n",
    "    'solution': {\n",
    "        'name': 'SVC', \n",
    "        'parameters': {\n",
    "            'C': 0.00035938136638046257, \n",
    "            'kernel': 'poly', \n",
    "            'gamma': 'scale'\n",
    "        }\n",
    "    }, \n",
    "    'score': 0.9907407407407407\n",
    "}\n",
    "\n",
    "digits_all = [{\n",
    "    'solution': {\n",
    "        'name': 'LogisticRegression', \n",
    "        'parameters': {\n",
    "            'penalty': 'l2', \n",
    "            'C': 0.00026366508987303583, \n",
    "            'solver': 'saga', \n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    }, \n",
    "    'score': 0.9722222222222222\n",
    "}, {\n",
    "    'solution': {\n",
    "        'name': 'MLPClassifier', \n",
    "        'parameters': {\n",
    "            'hidden_layer_sizes': (15,), \n",
    "            'activation': 'tanh', \n",
    "            'solver': 'sgd', \n",
    "            'alpha': 0.00035938136638046257, \n",
    "            'learning_rate': 'adaptive'\n",
    "        }\n",
    "    }, \n",
    "    'score': 0.987037037037037\n",
    "}, {\n",
    "    'solution': {\n",
    "        'name': 'SGDClassifier', \n",
    "        'parameters': {\n",
    "            'penalty': 'elasticnet', \n",
    "            'loss': 'hinge', \n",
    "            'alpha': 0.00035938136638046257, \n",
    "            'eta0': 0.001291549665014884, \n",
    "            'learning_rate': 'constant'\n",
    "        }\n",
    "    }, \n",
    "    'score': 0.9740740740740741\n",
    "}, {\n",
    "    'solution': {\n",
    "        'name': 'SVC', \n",
    "        'parameters': {\n",
    "            'C': 0.00035938136638046257, \n",
    "            'kernel': 'poly', \n",
    "            'gamma': 'scale'\n",
    "        }\n",
    "    }, \n",
    "    'score': 0.9907407407407407\n",
    "}, {\n",
    "    'solution': {\n",
    "        'name': 'RandomForestClassifier', \n",
    "        'parameters': {\n",
    "            'n_estimators': 16, \n",
    "            'criterion': 'gini', \n",
    "            'max_depth': 15, \n",
    "            'min_samples_split': 0.2\n",
    "        }\n",
    "    }, \n",
    "    'score': 0.9055555555555556\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# White Wine Dataset\n",
    "Whitewine_best_solution = { 'solution': \n",
    "                           { 'name': 'MLPClassifier', 'parameters': \n",
    "                            { 'hidden_layer_sizes': (68), 'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.016681005372000592, 'learning_rate': 'constant' }\n",
    "                           }, \n",
    "                           'score': 0.7520098948670377 }\n",
    "\n",
    "Whitewine_all_solutions = [\n",
    "    { 'solution': { 'name': 'LogisticRegression', 'parameters': { 'penalty': 'l1', 'C': 10000.0, 'solver': 'liblinear', 'n_jobs': -1 } }, 'score': 0.7235621521335807 }, \n",
    "    { 'solution': { 'name': 'MLPClassifier', 'parameters': { 'hidden_layer_sizes': (68), 'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.016681005372000592, 'learning_rate': 'constant' } }, 'score': 0.7520098948670377 }, \n",
    "    { 'solution': { 'name': 'SGDClassifier', 'parameters': { 'penalty': 'l1', 'loss': 'squared_loss', 'alpha': 0.004641588833612782, 'eta0': 0.0001, 'learning_rate': 'invscaling' } }, 'score': 0.7309833024118738 }, \n",
    "    { 'solution': { 'name': 'SVC', 'parameters': { 'C': 0.001291549665014884, 'kernel': 'poly', 'gamma': 'auto' } }, 'score': 0.7421150278293135 }, \n",
    "    { 'solution': { 'name': 'RandomForestClassifier', 'parameters': { 'n_estimators': 28, 'criterion': 'gini', 'max_depth': 15, 'min_samples_split': 0.1 } }, 'score': 0.7396413110698825 }\n",
    "]\n",
    "\n",
    "# TPot Score\n",
    "Score = \"0.769325912183055\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
